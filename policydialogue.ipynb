{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_pc = torch.tensor([1, 5 , 5,3,-1, 5, 5,3,1,-2 ])\n",
    "reward_npc = torch.tensor([0,-2,-3,-1,0, -2, -3, -1,1])\n",
    "#index = id-1\n",
    "# action_transform\n",
    "action_transform = torch.tensor([[1,3,6,7],[2,4,6,8],[1,5,6,7]])\n",
    "\n",
    "dialogue_mapping_NPC = {\n",
    "    1:\"hello witcher\",\n",
    "    2:\"hey kid!\",\n",
    "    3:\"How is it going on brave witcher lord?\",\n",
    "    4:\"Witcher can you save us from the creature thats attacking us?\",\n",
    "    5:\"Oe witcher,  defeat a monster for me.\",\n",
    "    6:\"Lord witcher can you save us from the creature that is attacking us?\",\n",
    "    7:\"We don't know what monster attacked us but it was vicious. It must have to huge as it slaughtered no less than a dozen wolves. Ripped their guts out, but left lost uneaten. Howls at nighhts. People are afraid to venture into the woods at night. Please lord witcher save us from the beast.\",\n",
    "    8:\" We don't know what monster attacked us but it was vicious. It must have to huge as it slaughtered no less than a dozen wolves. Ripped their guts out, but left lost uneaten. Howls at nighhts. People are afraid to venture into the woods at night.\",\n",
    "    9:\"No idea what monster attacked us if i knew i would be dead. It must have to huge as it slaughtered no less than a dozen wolves. Ripped their guts out, but left lost uneaten. You sure kid like you can handle it. Howls at nighhts. People are afraid to venture into the woods at night. Sure you can take care of it we dont want any more dead bodies.\"\n",
    "}\n",
    "\n",
    "\n",
    "dialogue_mapping_PC ={\n",
    "    1:\"Greetings\",\n",
    "    2:\"Fine just busy fighting creatures from Void dimension\",\n",
    "    3:\"Ok let me help you with it. What creature is it?\",\n",
    "    4:\" I m gonna help you but i need to paid well for a contract like that. What kind of creature is it\",\n",
    "    5:\"I don't have to bother with this work. farewell\",\n",
    "    6:\" Ok, sir let me help you. What kind of creature is it.\",\n",
    "    7:\"Sure it will take 500$ \",\n",
    "    8:\" Sure it will take 800$\",\n",
    "    9:\"Sure it will take 1200$\",\n",
    "    10:\"Sorry I am busy. I won't be able to do that.\"\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "def user_in(lister):\n",
    "   \n",
    "    \n",
    "    for i in lister:\n",
    "\n",
    "        print(dialogue_mapping_PC[i], i)\n",
    "    x=input()\n",
    "    return x\n",
    "\n",
    "def user_good(lister):\n",
    "   \n",
    "    if len(lister) == 3:\n",
    "        x= 3\n",
    "    else:\n",
    "        x= 7\n",
    "\n",
    "    return x\n",
    "\n",
    "def user_bad(lister):\n",
    "   \n",
    "    if len(lister) == 3:\n",
    "        x= 5\n",
    "    else:\n",
    "        x= 10 \n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork():\n",
    "    def __init__(self,n_state,n_action,n_hidden=50,lr=0.001):\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_state,n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden,n_action),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "\n",
    "    def predict(self,s):\n",
    "        return self.model(torch.Tensor(s))\n",
    "\n",
    "    def update(self, advantags, log_probs):\n",
    "        policy_gradient =[]\n",
    "        for log_prob , Gt in zip(log_probs, advantags):\n",
    "            policy_gradient.append(-log_prob* Gt)\n",
    "        \n",
    "        loss = torch.stack(policy_gradient).sum()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def get_action(self, s):\n",
    "        probs = self.predict(s)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        log_prob = torch.log(probs[action])\n",
    "        return action , log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork():\n",
    "    def __init__(self, n_state, n_hidden=50, lr = 0.05):\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_state, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden,1)\n",
    "\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),lr)\n",
    "\n",
    "    \n",
    "    def update(self, s, y):\n",
    "        y_pred = self.model(torch.Tensor(s))\n",
    "        loss = self.criterion(y_pred,Variable(torch.Tensor(y)))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def predict(self, s):\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.Tensor(s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment_user():\n",
    "    def __init__(self):\n",
    "        self.state=(0,0) #(NPC, PC)\n",
    "        self.superState = 0\n",
    "    def PC_action(self,action_transformed):\n",
    "        if self.superState ==0:\n",
    "            return 1 \n",
    "            ## code to ask input from the user\n",
    "            # transform into the id\n",
    "\n",
    "        elif self.superState == 1:\n",
    "            if action_transformed == 3:\n",
    "                return 2\n",
    "       \n",
    "            else:\n",
    "                lister = [3,4,5]\n",
    "                x= user_bad(lister)\n",
    "                return int(x)\n",
    "\n",
    "        elif self.superState ==2:\n",
    "            return 6\n",
    "\n",
    "        elif self.superState ==3 :\n",
    "            lister = [7,8,9,10]\n",
    "            x= user_bad(lister)\n",
    "            return int(x)  \n",
    "\n",
    "        else :\n",
    "            return -1\n",
    "        \n",
    "    def return_reward(self):\n",
    "        return reward_npc[self.state[0]-1] + reward_pc[self.state[1]-1]\n",
    "\n",
    "\n",
    "    def next_state(self,action_transformed ):\n",
    "        pc_action = self.PC_action(action_transformed)\n",
    "        if pc_action ==-1:\n",
    "            self.state =(-1,-1)\n",
    "            return self.state\n",
    "        self.state = (action_transformed,pc_action)\n",
    "        if self.superState == 0:\n",
    "            \n",
    "            self.superState =1\n",
    "        elif self.superState == 1:\n",
    "\n",
    "            if action_transformed == 3:\n",
    "\n",
    "                self.superState = 2\n",
    "            elif pc_action == 5:\n",
    "                self.superState =4\n",
    "\n",
    "            else :\n",
    "                self.superState=3\n",
    "\n",
    "\n",
    "        elif self.superState ==2:\n",
    "            self.superState = 3\n",
    "\n",
    "        elif self.superState ==3:\n",
    "            self.superState = 4\n",
    "\n",
    "        else:\n",
    "            self.superState = 4\n",
    "        \n",
    "        return self.state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env,estimator_policy,estimator_value, n_episode, gamma =1):\n",
    "    for episode in range(n_episode):\n",
    "        log_probs =[]\n",
    "        states =[]\n",
    "        rewards = []\n",
    "        is_done = False\n",
    "        env.state=(0,0)\n",
    "        env.superState = 0\n",
    "        state = env.state\n",
    "\n",
    "        while True:\n",
    "            states.append(state)\n",
    "            action, log_prob = estimator_policy.get_action(state)\n",
    "            transform_action = action_transform[action,env.superState]\n",
    "            #print(\"NPC:\",dialogue_mapping_NPC[transform_action.item()])\n",
    "            next_state = env.next_state(transform_action)\n",
    "            if next_state == (-1,-1):\n",
    "                is_done = True\n",
    "            if env.superState == 4:\n",
    "                is_done = True\n",
    "            #print(\"PC:\",dialogue_mapping_PC[next_state[1]])\n",
    "#            pause = input()\n",
    "            reward =env.return_reward()\n",
    "\n",
    "            total_reward_episode[episode] += reward\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if is_done:\n",
    "                Gt=0 \n",
    "                pw = 0 \n",
    "                returns = []\n",
    "                for t in range(len(states)-1,-1,-1):\n",
    "                    Gt += gamma ** pw * rewards[t]\n",
    "                    pw += 1\n",
    "                    returns.append(Gt)\n",
    "\n",
    "                returns = returns[::-1]\n",
    "                returns = torch.tensor(returns)\n",
    "                baseline_values = estimator_value.predict(states)\n",
    "\n",
    "                advantages = returns - baseline_values\n",
    "                estimator_value.update(states,returns)\n",
    "                estimator_policy.update(advantages, log_probs)\n",
    "                print('Episode:{}, total reward: {}'.format(episode, total_reward_episode[episode]))\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "\n",
    "                \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_state  =2\n",
    "n_action = 3\n",
    "n_hidden_p = 64\n",
    "lr_p = 0.003\n",
    "policy_net = PolicyNetwork(n_state, n_action, n_hidden_p , lr_p)\n",
    "\n",
    "n_hidden_v = 64\n",
    "lr_v = 0.003\n",
    "value_net = ValueNetwork(n_state, n_hidden_v, lr_v)\n",
    "gamma=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prayash/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "/home/prayash/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/prayash/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:0, total reward: 3\n",
      "Episode:1, total reward: -1\n",
      "Episode:2, total reward: 1\n",
      "Episode:3, total reward: 3\n",
      "Episode:4, total reward: 0\n",
      "Episode:5, total reward: 1\n",
      "Episode:6, total reward: -2\n",
      "Episode:7, total reward: 1\n",
      "Episode:8, total reward: 3\n",
      "Episode:9, total reward: 0\n",
      "Episode:10, total reward: -1\n",
      "Episode:11, total reward: 1\n",
      "Episode:12, total reward: 1\n",
      "Episode:13, total reward: 3\n",
      "Episode:14, total reward: -1\n",
      "Episode:15, total reward: -1\n",
      "Episode:16, total reward: -1\n",
      "Episode:17, total reward: -3\n",
      "Episode:18, total reward: -1\n",
      "Episode:19, total reward: 3\n",
      "Episode:20, total reward: -1\n",
      "Episode:21, total reward: 3\n",
      "Episode:22, total reward: 3\n",
      "Episode:23, total reward: 1\n",
      "Episode:24, total reward: -1\n",
      "Episode:25, total reward: 3\n",
      "Episode:26, total reward: -3\n",
      "Episode:27, total reward: 0\n",
      "Episode:28, total reward: 3\n",
      "Episode:29, total reward: -1\n",
      "Episode:30, total reward: 0\n",
      "Episode:31, total reward: 0\n",
      "Episode:32, total reward: -2\n",
      "Episode:33, total reward: -1\n",
      "Episode:34, total reward: 3\n",
      "Episode:35, total reward: 0\n",
      "Episode:36, total reward: -1\n",
      "Episode:37, total reward: 0\n",
      "Episode:38, total reward: 0\n",
      "Episode:39, total reward: 3\n",
      "Episode:40, total reward: 0\n",
      "Episode:41, total reward: -1\n",
      "Episode:42, total reward: 1\n",
      "Episode:43, total reward: -1\n",
      "Episode:44, total reward: 1\n",
      "Episode:45, total reward: 3\n",
      "Episode:46, total reward: 3\n",
      "Episode:47, total reward: 3\n",
      "Episode:48, total reward: 3\n",
      "Episode:49, total reward: 3\n",
      "Episode:50, total reward: 3\n",
      "Episode:51, total reward: 3\n",
      "Episode:52, total reward: 0\n",
      "Episode:53, total reward: 3\n",
      "Episode:54, total reward: -1\n",
      "Episode:55, total reward: -1\n",
      "Episode:56, total reward: 1\n",
      "Episode:57, total reward: -1\n",
      "Episode:58, total reward: -1\n",
      "Episode:59, total reward: 3\n",
      "Episode:60, total reward: -1\n",
      "Episode:61, total reward: -1\n",
      "Episode:62, total reward: 3\n",
      "Episode:63, total reward: 1\n",
      "Episode:64, total reward: -1\n",
      "Episode:65, total reward: 3\n",
      "Episode:66, total reward: 3\n",
      "Episode:67, total reward: -1\n",
      "Episode:68, total reward: 3\n",
      "Episode:69, total reward: 3\n",
      "Episode:70, total reward: -3\n",
      "Episode:71, total reward: 3\n",
      "Episode:72, total reward: 3\n",
      "Episode:73, total reward: 3\n",
      "Episode:74, total reward: 3\n",
      "Episode:75, total reward: 3\n",
      "Episode:76, total reward: 3\n",
      "Episode:77, total reward: 3\n",
      "Episode:78, total reward: 3\n",
      "Episode:79, total reward: 3\n",
      "Episode:80, total reward: 3\n",
      "Episode:81, total reward: 3\n",
      "Episode:82, total reward: -1\n",
      "Episode:83, total reward: 1\n",
      "Episode:84, total reward: 3\n",
      "Episode:85, total reward: 3\n",
      "Episode:86, total reward: 3\n",
      "Episode:87, total reward: 3\n",
      "Episode:88, total reward: -1\n",
      "Episode:89, total reward: 3\n",
      "Episode:90, total reward: -1\n",
      "Episode:91, total reward: 3\n",
      "Episode:92, total reward: 3\n",
      "Episode:93, total reward: 3\n",
      "Episode:94, total reward: 3\n",
      "Episode:95, total reward: 3\n",
      "Episode:96, total reward: 3\n",
      "Episode:97, total reward: 3\n",
      "Episode:98, total reward: -1\n",
      "Episode:99, total reward: 3\n",
      "Episode:100, total reward: 3\n",
      "Episode:101, total reward: 3\n",
      "Episode:102, total reward: 3\n",
      "Episode:103, total reward: 1\n",
      "Episode:104, total reward: 3\n",
      "Episode:105, total reward: 3\n",
      "Episode:106, total reward: 3\n",
      "Episode:107, total reward: 3\n",
      "Episode:108, total reward: 3\n",
      "Episode:109, total reward: 3\n",
      "Episode:110, total reward: 3\n",
      "Episode:111, total reward: 0\n",
      "Episode:112, total reward: 3\n",
      "Episode:113, total reward: 3\n",
      "Episode:114, total reward: 3\n",
      "Episode:115, total reward: 3\n",
      "Episode:116, total reward: 3\n",
      "Episode:117, total reward: 3\n",
      "Episode:118, total reward: 3\n",
      "Episode:119, total reward: 3\n",
      "Episode:120, total reward: 3\n",
      "Episode:121, total reward: 3\n",
      "Episode:122, total reward: 3\n",
      "Episode:123, total reward: 3\n",
      "Episode:124, total reward: 3\n",
      "Episode:125, total reward: -1\n",
      "Episode:126, total reward: 3\n",
      "Episode:127, total reward: 3\n",
      "Episode:128, total reward: 3\n",
      "Episode:129, total reward: 3\n",
      "Episode:130, total reward: 3\n",
      "Episode:131, total reward: 3\n",
      "Episode:132, total reward: 3\n",
      "Episode:133, total reward: 3\n",
      "Episode:134, total reward: 3\n",
      "Episode:135, total reward: 3\n",
      "Episode:136, total reward: 3\n",
      "Episode:137, total reward: 3\n",
      "Episode:138, total reward: 3\n",
      "Episode:139, total reward: -1\n",
      "Episode:140, total reward: 3\n",
      "Episode:141, total reward: 3\n",
      "Episode:142, total reward: 3\n",
      "Episode:143, total reward: 3\n",
      "Episode:144, total reward: 3\n",
      "Episode:145, total reward: 3\n",
      "Episode:146, total reward: -1\n",
      "Episode:147, total reward: 3\n",
      "Episode:148, total reward: 3\n",
      "Episode:149, total reward: 3\n",
      "Episode:150, total reward: 3\n",
      "Episode:151, total reward: 3\n",
      "Episode:152, total reward: 3\n",
      "Episode:153, total reward: 3\n",
      "Episode:154, total reward: 3\n",
      "Episode:155, total reward: 3\n",
      "Episode:156, total reward: 3\n",
      "Episode:157, total reward: 3\n",
      "Episode:158, total reward: 3\n",
      "Episode:159, total reward: 3\n",
      "Episode:160, total reward: 3\n",
      "Episode:161, total reward: 3\n",
      "Episode:162, total reward: 3\n",
      "Episode:163, total reward: 3\n",
      "Episode:164, total reward: 3\n",
      "Episode:165, total reward: 3\n",
      "Episode:166, total reward: 3\n",
      "Episode:167, total reward: 3\n",
      "Episode:168, total reward: 3\n",
      "Episode:169, total reward: 1\n",
      "Episode:170, total reward: 3\n",
      "Episode:171, total reward: 3\n",
      "Episode:172, total reward: 3\n",
      "Episode:173, total reward: 3\n",
      "Episode:174, total reward: 3\n",
      "Episode:175, total reward: -1\n",
      "Episode:176, total reward: 3\n",
      "Episode:177, total reward: 3\n",
      "Episode:178, total reward: -1\n",
      "Episode:179, total reward: 3\n",
      "Episode:180, total reward: 3\n",
      "Episode:181, total reward: 3\n",
      "Episode:182, total reward: 3\n",
      "Episode:183, total reward: 3\n",
      "Episode:184, total reward: 3\n",
      "Episode:185, total reward: 3\n",
      "Episode:186, total reward: 3\n",
      "Episode:187, total reward: 3\n",
      "Episode:188, total reward: 3\n",
      "Episode:189, total reward: 3\n",
      "Episode:190, total reward: 3\n",
      "Episode:191, total reward: 3\n",
      "Episode:192, total reward: 3\n",
      "Episode:193, total reward: 3\n",
      "Episode:194, total reward: 3\n",
      "Episode:195, total reward: 3\n",
      "Episode:196, total reward: 3\n",
      "Episode:197, total reward: 3\n",
      "Episode:198, total reward: 3\n",
      "Episode:199, total reward: 3\n",
      "Episode:200, total reward: 3\n",
      "Episode:201, total reward: 3\n",
      "Episode:202, total reward: 3\n",
      "Episode:203, total reward: 3\n",
      "Episode:204, total reward: 3\n",
      "Episode:205, total reward: 3\n",
      "Episode:206, total reward: 3\n",
      "Episode:207, total reward: 3\n",
      "Episode:208, total reward: 3\n",
      "Episode:209, total reward: 3\n",
      "Episode:210, total reward: 3\n",
      "Episode:211, total reward: 3\n",
      "Episode:212, total reward: 3\n",
      "Episode:213, total reward: 3\n",
      "Episode:214, total reward: 3\n",
      "Episode:215, total reward: 3\n",
      "Episode:216, total reward: 3\n",
      "Episode:217, total reward: 3\n",
      "Episode:218, total reward: 3\n",
      "Episode:219, total reward: 3\n",
      "Episode:220, total reward: 3\n",
      "Episode:221, total reward: 3\n",
      "Episode:222, total reward: 3\n",
      "Episode:223, total reward: 3\n",
      "Episode:224, total reward: 3\n",
      "Episode:225, total reward: 3\n",
      "Episode:226, total reward: 3\n",
      "Episode:227, total reward: 3\n",
      "Episode:228, total reward: 3\n",
      "Episode:229, total reward: 3\n",
      "Episode:230, total reward: 3\n",
      "Episode:231, total reward: 3\n",
      "Episode:232, total reward: 3\n",
      "Episode:233, total reward: 3\n",
      "Episode:234, total reward: 3\n",
      "Episode:235, total reward: 3\n",
      "Episode:236, total reward: 3\n",
      "Episode:237, total reward: 3\n",
      "Episode:238, total reward: 3\n",
      "Episode:239, total reward: 3\n",
      "Episode:240, total reward: 3\n",
      "Episode:241, total reward: 3\n",
      "Episode:242, total reward: -1\n",
      "Episode:243, total reward: 3\n",
      "Episode:244, total reward: 3\n",
      "Episode:245, total reward: 3\n",
      "Episode:246, total reward: 3\n",
      "Episode:247, total reward: 3\n",
      "Episode:248, total reward: 3\n",
      "Episode:249, total reward: 3\n",
      "Episode:250, total reward: 3\n",
      "Episode:251, total reward: 3\n",
      "Episode:252, total reward: 3\n",
      "Episode:253, total reward: 3\n",
      "Episode:254, total reward: 3\n",
      "Episode:255, total reward: 3\n",
      "Episode:256, total reward: 3\n",
      "Episode:257, total reward: 3\n",
      "Episode:258, total reward: 3\n",
      "Episode:259, total reward: 3\n",
      "Episode:260, total reward: 3\n",
      "Episode:261, total reward: 3\n",
      "Episode:262, total reward: -1\n",
      "Episode:263, total reward: 3\n",
      "Episode:264, total reward: 3\n",
      "Episode:265, total reward: 3\n",
      "Episode:266, total reward: 3\n",
      "Episode:267, total reward: 3\n",
      "Episode:268, total reward: 3\n",
      "Episode:269, total reward: 3\n",
      "Episode:270, total reward: 3\n",
      "Episode:271, total reward: 3\n",
      "Episode:272, total reward: 3\n",
      "Episode:273, total reward: -1\n",
      "Episode:274, total reward: 3\n",
      "Episode:275, total reward: 3\n",
      "Episode:276, total reward: 3\n",
      "Episode:277, total reward: 3\n",
      "Episode:278, total reward: 3\n",
      "Episode:279, total reward: 3\n",
      "Episode:280, total reward: 3\n",
      "Episode:281, total reward: 3\n",
      "Episode:282, total reward: 0\n",
      "Episode:283, total reward: 3\n",
      "Episode:284, total reward: 3\n",
      "Episode:285, total reward: 3\n",
      "Episode:286, total reward: 3\n",
      "Episode:287, total reward: 0\n",
      "Episode:288, total reward: 3\n",
      "Episode:289, total reward: 3\n",
      "Episode:290, total reward: 3\n",
      "Episode:291, total reward: 3\n",
      "Episode:292, total reward: 3\n",
      "Episode:293, total reward: 3\n",
      "Episode:294, total reward: 3\n",
      "Episode:295, total reward: 3\n",
      "Episode:296, total reward: 3\n",
      "Episode:297, total reward: 3\n",
      "Episode:298, total reward: 3\n",
      "Episode:299, total reward: 3\n"
     ]
    }
   ],
   "source": [
    "n_episode = 300\n",
    "total_reward_episode = [0] * n_episode\n",
    "reinforce(env, policy_net, value_net, n_episode, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
